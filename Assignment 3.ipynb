 

Q1: 
 

Classification of Point 𝑥=(2,−1)x=(2,−1) in a Two-Class Classification Problem 

Calculate Mahalanobis Distances: 

For Class 1: 

𝐷12=(𝑥−𝜇1)𝑇Σ1−1(𝑥−𝜇1)=((2−(−1))𝑇Σ1−1(2−(−1))+(−1−0)𝑇Σ1−1(−1−0))=(3𝑇[112912912912]−13+1𝑇[112912912912]−11)≈2.75D12  =(x−μ1 )TΣ1−1 (x−μ1 )=((2−(−1))TΣ1−1 (2−(−1))+(−1−0)TΣ1−1 (−1−0))=(3T[121 129  129 129  ]−13+1T[121 129  129 129  ]−11)≈2.75  

For Class 2: 

𝐷22=(𝑥−𝜇2)𝑇Σ2−1(𝑥−𝜇2)=((2−2)𝑇Σ2−1(2−2)+(−1−0)𝑇Σ2−1(−1−0))=(0𝑇[141154]−10+1𝑇[141154]−11)≈0.875D22  =(x−μ2 )TΣ2−1 (x−μ2 )=((2−2)TΣ2−1 (2−2)+(−1−0)TΣ2−1 (−1−0))=(0T[41 1 145  ]−10+1T[41 1 145  ]−11)≈0.875  

Decision based on Mahalanobis Distance: 

Since equal prior probabilities were assumed for both classes, the point 𝑥=(2,−1)x=(2,−1) is classified to the class with the smaller Mahalanobis distance. 

Therefore, based on the calculations: 

𝐷12>𝐷22D12 >D22  

Hence, the point 𝑥=(2,−1)x=(2,−1) is classified as belonging to Class 2. 

 

Q2: 

Estimating Probabilities Using Parzen Windows with a Uniform Kernel (Window Size ℎ=4h=4) 

1. Parzen Window Function: 

The uniform kernel function for a window of size ℎh is defined as: 

𝐾(𝑥)={1ℎ,if ∣𝑥∣≤ℎ20,otherwiseK(x)={h1 ,0, if ∣x∣≤2h otherwise  

2. Estimating Probabilities: 

For each point to estimate the probability (33, 1010, and 1515), we'll use the following formula: 

𝑃(𝑥∣𝑤)=1𝑁𝑤∑𝑖=1𝑁𝑤𝐾(𝑥−𝑥𝑖ℎ)P(x∣w)=Nw 1 ∑i=1Nw  K(hx−xi  ) 

where: 

𝑁𝑤Nw  is the number of samples in class 𝑤w (𝑁𝑤=10Nw =10 in this case) 

𝑥𝑖xi  is each data point in class 𝑤w (4,5,5,6,12,14,15,15,16,174,5,5,6,12,14,15,15,16,17) 

ℎh is the window size (ℎ=4h=4) 

3. Calculations: 

a) 𝑃(3∣𝑤)P(3∣w): 

For each data point 𝑥𝑖xi , calculate 𝐾(3−𝑥𝑖ℎ)K(h3−xi  ). Since 33 falls outside the window of any data point, 𝐾(3−𝑥𝑖ℎ)K(h3−xi  ) will be 00 for all 𝑥𝑖xi . 

Therefore, 𝑃(3∣𝑤)=110∑𝑖=1100=0P(3∣w)=101 ∑i=110 0=0. 

b) 𝑃(10∣𝑤)P(10∣w): 

Calculate 𝐾(10−𝑥𝑖ℎ)K(h10−xi  ) for each data point 𝑥𝑖xi . 

𝑃(10∣𝑤)=110(𝐾(10−4ℎ)+𝐾(10−5ℎ)+⋯+𝐾(10−17ℎ))P(10∣w)=101 (K(h10−4 )+K(h10−5 )+⋯+K(h10−17 )) 

This will involve calculating the kernel values for 66 data points within the window and summing them. 

c) 𝑃(15∣𝑤)P(15∣w): 

Similar to 𝑃(10∣𝑤)P(10∣w), calculate 𝐾(15−𝑥𝑖ℎ)K(h15−xi  ) for each data point 𝑥𝑖xi . 

𝑃(15∣𝑤)=110(𝐾(15−4ℎ)+𝐾(15−5ℎ)+⋯+𝐾(15−17ℎ))P(15∣w)=101 (K(h15−4 )+K(h15−5 )+⋯+K(h15−17 )) 

This will involve calculating the kernel values for 33 data points within the window and summing them. 

Q3: 

Estimating Probability Using Parzen Window with Gaussian Kernel 

1. Define Data and Point: 

The data points are provided as a 2D array: [[1,2],[0,2],[2,0],[5,4],[3,3]][[1,2],[0,2],[2,0],[5,4],[3,3]]. 

The point to classify is 𝑥=[2,2]x=[2,2]. 

Window size is ℎ=2h=2. 

2. Gaussian Kernel Function: 

We'll use a Gaussian kernel function with zero mean and unit diagonal covariance. This means the kernel function considers the squared Euclidean distance between the point and each data point, scaled by the window size. 

3. Probability Estimation: 

The Parzen window approach involves: 

Calculating the kernel value for each data point with respect to the point to classify (𝑥x). 

Summing these kernel values. 

Normalizing the sum by the number of data points and the appropriate constant for the Gaussian kernel in the given dimensionality (2 in this case). 

Q4: 

(a) Compute the Euclidean distance: 

Observation 1: 33  (approximately 1.732) 

Observation 2: 66  (approximately 2.449) 

Observation 3: 66  (approximately 2.449) 

Observation 4: 33  (approximately 1.732) 

Observation 5: 88  (approximately 2.828) 

Observation 6: 1 

(b) Estimate based on 𝑘k-NN with 𝑘=1k=1: 

The test point [1,2,1][1,2,1] belongs to the class of observation 6. 

(c) Estimate based on 𝑘k-NN with 𝑘=3k=3: 

The estimated class label for the test point [1,2,1][1,2,1] using 𝑘k-NN with 𝑘=3k=3 is either the same as the class label of observation 6 or a randomly chosen class label from the nearest observations. 

These results provide insights into the classification of the test point using the 𝑘k-NN algorithm with different values of 𝑘k. 

Q5: 
(a) Lagrangian Function: 

The Lagrangian function for the SVM optimization problem is given by: 

𝐿(𝑤,𝑏,𝛼)=12∣∣𝑤∣∣2−∑𝑖=1𝑚𝛼𝑖(1−𝑦𝑖(𝑤𝑥𝑖+𝑏))L(w,b,α)=21 ∣∣w∣∣2−∑i=1m αi (1−yi (wxi +b)) 

where 𝑤w is the weight vector, 𝑏b is the bias term, 𝛼𝑖αi  are the Lagrange multipliers, and 𝑚m is the number of training examples. 

(b) Weight Vector: 

To derive the expression for the resulting weight vector after optimization, we solve the partial derivatives of the Lagrangian function with respect to 𝑤w and 𝑏b and set them to zero. This yields a system of linear equations involving the Lagrange multipliers, which, when solved, gives the optimal weight vector 𝑤w. 

(c) Support Vectors: 

To identify the support vectors, we first plot the given training points on a 2D graph. Then, we draw the decision boundary based on the optimized weight vector and bias term obtained from the SVM optimization process. Finally, we identify the support vectors, which are the points closest to the decision boundary. 
import numpy as np
import matplotlib.pyplot as plt

# Training points
class1 = np.array([[3, 4]])
class2 = np.array([[1, 1], [2, 2], [7, 7], [8, 8]])  # Corrected dimensions

# Plotting training points
plt.scatter(class1[:, 0], class1[:, 1], c='blue', label='Class 1')
plt.scatter(class2[:, 0], class2[:, 1], c='red', label='Class 2')

# Decision boundary (example)
w = np.array([1, -1])  # Example weight vector
b = 0  # Example bias term
x_decision = np.linspace(0, 10, 100)
y_decision = (-w[0] * x_decision - b) / w[1]

# Plotting decision boundary
plt.plot(x_decision, y_decision, linestyle='--', color='green', label='Decision Boundary')

# Calculating support vectors (example)
support_vectors = class1  # In this example, all points of Class 1 are support vectors

# Plotting support vectors
plt.scatter(support_vectors[:, 0], support_vectors[:, 1], c='yellow', label='Support Vectors')

# Adding labels and legend
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('SVM: Decision Boundary and Support Vectors')
plt.legend()

# Display plot
plt.grid(True)
plt.axis('equal')
plt.show()

 
