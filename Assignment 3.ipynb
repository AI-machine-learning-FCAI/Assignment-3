 

Q1: 
 

Classification of Point ğ‘¥=(2,âˆ’1)x=(2,âˆ’1) in a Two-Class Classification Problem 

Calculate Mahalanobis Distances: 

For Class 1: 

ğ·12=(ğ‘¥âˆ’ğœ‡1)ğ‘‡Î£1âˆ’1(ğ‘¥âˆ’ğœ‡1)=((2âˆ’(âˆ’1))ğ‘‡Î£1âˆ’1(2âˆ’(âˆ’1))+(âˆ’1âˆ’0)ğ‘‡Î£1âˆ’1(âˆ’1âˆ’0))=(3ğ‘‡[112912912912]âˆ’13+1ğ‘‡[112912912912]âˆ’11)â‰ˆ2.75D12  =(xâˆ’Î¼1 )TÎ£1âˆ’1 (xâˆ’Î¼1 )=((2âˆ’(âˆ’1))TÎ£1âˆ’1 (2âˆ’(âˆ’1))+(âˆ’1âˆ’0)TÎ£1âˆ’1 (âˆ’1âˆ’0))=(3T[121 129  129 129  ]âˆ’13+1T[121 129  129 129  ]âˆ’11)â‰ˆ2.75  

For Class 2: 

ğ·22=(ğ‘¥âˆ’ğœ‡2)ğ‘‡Î£2âˆ’1(ğ‘¥âˆ’ğœ‡2)=((2âˆ’2)ğ‘‡Î£2âˆ’1(2âˆ’2)+(âˆ’1âˆ’0)ğ‘‡Î£2âˆ’1(âˆ’1âˆ’0))=(0ğ‘‡[141154]âˆ’10+1ğ‘‡[141154]âˆ’11)â‰ˆ0.875D22  =(xâˆ’Î¼2 )TÎ£2âˆ’1 (xâˆ’Î¼2 )=((2âˆ’2)TÎ£2âˆ’1 (2âˆ’2)+(âˆ’1âˆ’0)TÎ£2âˆ’1 (âˆ’1âˆ’0))=(0T[41 1 145  ]âˆ’10+1T[41 1 145  ]âˆ’11)â‰ˆ0.875  

Decision based on Mahalanobis Distance: 

Since equal prior probabilities were assumed for both classes, the point ğ‘¥=(2,âˆ’1)x=(2,âˆ’1) is classified to the class with the smaller Mahalanobis distance. 

Therefore, based on the calculations: 

ğ·12>ğ·22D12 >D22  

Hence, the point ğ‘¥=(2,âˆ’1)x=(2,âˆ’1) is classified as belonging to Class 2. 

 

Q2: 

Estimating Probabilities Using Parzen Windows with a Uniform Kernel (Window Size â„=4h=4) 

1. Parzen Window Function: 

The uniform kernel function for a window of size â„h is defined as: 

ğ¾(ğ‘¥)={1â„,if âˆ£ğ‘¥âˆ£â‰¤â„20,otherwiseK(x)={h1 ,0, if âˆ£xâˆ£â‰¤2h otherwise  

2. Estimating Probabilities: 

For each point to estimate the probability (33, 1010, and 1515), we'll use the following formula: 

ğ‘ƒ(ğ‘¥âˆ£ğ‘¤)=1ğ‘ğ‘¤âˆ‘ğ‘–=1ğ‘ğ‘¤ğ¾(ğ‘¥âˆ’ğ‘¥ğ‘–â„)P(xâˆ£w)=Nw 1 âˆ‘i=1Nw  K(hxâˆ’xi  ) 

where: 

ğ‘ğ‘¤Nw  is the number of samples in class ğ‘¤w (ğ‘ğ‘¤=10Nw =10 in this case) 

ğ‘¥ğ‘–xi  is each data point in class ğ‘¤w (4,5,5,6,12,14,15,15,16,174,5,5,6,12,14,15,15,16,17) 

â„h is the window size (â„=4h=4) 

3. Calculations: 

a) ğ‘ƒ(3âˆ£ğ‘¤)P(3âˆ£w): 

For each data point ğ‘¥ğ‘–xi , calculate ğ¾(3âˆ’ğ‘¥ğ‘–â„)K(h3âˆ’xi  ). Since 33 falls outside the window of any data point, ğ¾(3âˆ’ğ‘¥ğ‘–â„)K(h3âˆ’xi  ) will be 00 for all ğ‘¥ğ‘–xi . 

Therefore, ğ‘ƒ(3âˆ£ğ‘¤)=110âˆ‘ğ‘–=1100=0P(3âˆ£w)=101 âˆ‘i=110 0=0. 

b) ğ‘ƒ(10âˆ£ğ‘¤)P(10âˆ£w): 

Calculate ğ¾(10âˆ’ğ‘¥ğ‘–â„)K(h10âˆ’xi  ) for each data point ğ‘¥ğ‘–xi . 

ğ‘ƒ(10âˆ£ğ‘¤)=110(ğ¾(10âˆ’4â„)+ğ¾(10âˆ’5â„)+â‹¯+ğ¾(10âˆ’17â„))P(10âˆ£w)=101 (K(h10âˆ’4 )+K(h10âˆ’5 )+â‹¯+K(h10âˆ’17 )) 

This will involve calculating the kernel values for 66 data points within the window and summing them. 

c) ğ‘ƒ(15âˆ£ğ‘¤)P(15âˆ£w): 

Similar to ğ‘ƒ(10âˆ£ğ‘¤)P(10âˆ£w), calculate ğ¾(15âˆ’ğ‘¥ğ‘–â„)K(h15âˆ’xi  ) for each data point ğ‘¥ğ‘–xi . 

ğ‘ƒ(15âˆ£ğ‘¤)=110(ğ¾(15âˆ’4â„)+ğ¾(15âˆ’5â„)+â‹¯+ğ¾(15âˆ’17â„))P(15âˆ£w)=101 (K(h15âˆ’4 )+K(h15âˆ’5 )+â‹¯+K(h15âˆ’17 )) 

This will involve calculating the kernel values for 33 data points within the window and summing them. 

Q3: 

Estimating Probability Using Parzen Window with Gaussian Kernel 

1. Define Data and Point: 

The data points are provided as a 2D array: [[1,2],[0,2],[2,0],[5,4],[3,3]][[1,2],[0,2],[2,0],[5,4],[3,3]]. 

The point to classify is ğ‘¥=[2,2]x=[2,2]. 

Window size is â„=2h=2. 

2. Gaussian Kernel Function: 

We'll use a Gaussian kernel function with zero mean and unit diagonal covariance. This means the kernel function considers the squared Euclidean distance between the point and each data point, scaled by the window size. 

3. Probability Estimation: 

The Parzen window approach involves: 

Calculating the kernel value for each data point with respect to the point to classify (ğ‘¥x). 

Summing these kernel values. 

Normalizing the sum by the number of data points and the appropriate constant for the Gaussian kernel in the given dimensionality (2 in this case). 

Q4: 

(a) Compute the Euclidean distance: 

Observation 1: 33  (approximately 1.732) 

Observation 2: 66  (approximately 2.449) 

Observation 3: 66  (approximately 2.449) 

Observation 4: 33  (approximately 1.732) 

Observation 5: 88  (approximately 2.828) 

Observation 6: 1 

(b) Estimate based on ğ‘˜k-NN with ğ‘˜=1k=1: 

The test point [1,2,1][1,2,1] belongs to the class of observation 6. 

(c) Estimate based on ğ‘˜k-NN with ğ‘˜=3k=3: 

The estimated class label for the test point [1,2,1][1,2,1] using ğ‘˜k-NN with ğ‘˜=3k=3 is either the same as the class label of observation 6 or a randomly chosen class label from the nearest observations. 

These results provide insights into the classification of the test point using the ğ‘˜k-NN algorithm with different values of ğ‘˜k. 

Q5: 
(a) Lagrangian Function: 

The Lagrangian function for the SVM optimization problem is given by: 

ğ¿(ğ‘¤,ğ‘,ğ›¼)=12âˆ£âˆ£ğ‘¤âˆ£âˆ£2âˆ’âˆ‘ğ‘–=1ğ‘šğ›¼ğ‘–(1âˆ’ğ‘¦ğ‘–(ğ‘¤ğ‘¥ğ‘–+ğ‘))L(w,b,Î±)=21 âˆ£âˆ£wâˆ£âˆ£2âˆ’âˆ‘i=1m Î±i (1âˆ’yi (wxi +b)) 

where ğ‘¤w is the weight vector, ğ‘b is the bias term, ğ›¼ğ‘–Î±i  are the Lagrange multipliers, and ğ‘šm is the number of training examples. 

(b) Weight Vector: 

To derive the expression for the resulting weight vector after optimization, we solve the partial derivatives of the Lagrangian function with respect to ğ‘¤w and ğ‘b and set them to zero. This yields a system of linear equations involving the Lagrange multipliers, which, when solved, gives the optimal weight vector ğ‘¤w. 

(c) Support Vectors: 

To identify the support vectors, we first plot the given training points on a 2D graph. Then, we draw the decision boundary based on the optimized weight vector and bias term obtained from the SVM optimization process. Finally, we identify the support vectors, which are the points closest to the decision boundary. 
import numpy as np
import matplotlib.pyplot as plt

# Training points
class1 = np.array([[3, 4]])
class2 = np.array([[1, 1], [2, 2], [7, 7], [8, 8]])  # Corrected dimensions

# Plotting training points
plt.scatter(class1[:, 0], class1[:, 1], c='blue', label='Class 1')
plt.scatter(class2[:, 0], class2[:, 1], c='red', label='Class 2')

# Decision boundary (example)
w = np.array([1, -1])  # Example weight vector
b = 0  # Example bias term
x_decision = np.linspace(0, 10, 100)
y_decision = (-w[0] * x_decision - b) / w[1]

# Plotting decision boundary
plt.plot(x_decision, y_decision, linestyle='--', color='green', label='Decision Boundary')

# Calculating support vectors (example)
support_vectors = class1  # In this example, all points of Class 1 are support vectors

# Plotting support vectors
plt.scatter(support_vectors[:, 0], support_vectors[:, 1], c='yellow', label='Support Vectors')

# Adding labels and legend
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('SVM: Decision Boundary and Support Vectors')
plt.legend()

# Display plot
plt.grid(True)
plt.axis('equal')
plt.show()

 
